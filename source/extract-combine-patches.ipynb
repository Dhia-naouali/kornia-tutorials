{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34fb3a3e",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.sandbox.google.com/github/kornia/tutorials/blob/master/source/extract-combine-patches.ipynb)\n",
    "\n",
    "# Extracting and Combining Tensor Patches\n",
    "\n",
    "In this tutorial we will show how you can extract and combine tensor patches using kornia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb4bd6d",
   "metadata": {},
   "source": [
    "## Using Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7419ae3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 8, 8])\n",
      "torch.Size([2, 9, 3, 4, 4])\n",
      "torch.Size([2, 3, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from kornia.contrib import CombineTensorPatches, ExtractTensorPatches\n",
    "\n",
    "h, w = 8, 8\n",
    "win = 4\n",
    "pad = 2\n",
    "\n",
    "image = torch.randn(2, 3, h, w)\n",
    "print(image.shape)\n",
    "tiler = ExtractTensorPatches(window_size=win, stride=win, padding=pad)\n",
    "merger = CombineTensorPatches(original_size=(h, w), window_size=win,  unpadding=pad)\n",
    "image_tiles = tiler(image)\n",
    "print(image_tiles.shape)\n",
    "new_image = merger(image_tiles)\n",
    "print(new_image.shape)\n",
    "assert (image == new_image).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e9bd7e",
   "metadata": {},
   "source": [
    "## Using Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ad0bae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 8, 8])\n",
      "torch.Size([1, 9, 1, 4, 4])\n",
      "torch.Size([1, 1, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from kornia.contrib import combine_tensor_patches, extract_tensor_patches\n",
    "\n",
    "h, w = 8, 8\n",
    "win = 4\n",
    "pad = 2\n",
    "\n",
    "image = torch.randn(1, 1, h, w)\n",
    "print(image.shape)\n",
    "patches = extract_tensor_patches(image, window_size=win, stride=win, padding=pad)\n",
    "print(patches.shape)\n",
    "restored_img = combine_tensor_patches(patches, original_size=(h, w), window_size=win,  stride=win, unpadding=pad)\n",
    "print(restored_img.shape)\n",
    "assert (image == restored_img).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc72717",
   "metadata": {},
   "source": [
    "## Important cases to consider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672fb037",
   "metadata": {},
   "source": [
    "While using these functions, it is important to keep track of the following points:\n",
    "\n",
    "1. Original image dimensions prior to extraction must be divisible by 2\n",
    "2. Image after padding must be divisible by window_size \n",
    "3. CombineTensorPatches only works with stride == window_size\n",
    "\n",
    "We will now examine the cases 1 and 2 and how to address them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01add821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_combine(image, window_size, padding):\n",
    "    h, w = image.shape[-2:]\n",
    "    tiler = ExtractTensorPatches(window_size=window_size, stride=window_size, padding=padding)\n",
    "    merger = CombineTensorPatches(original_size=(h, w), window_size=window_size, unpadding=padding)\n",
    "    image_tiles = tiler(image)\n",
    "    print(f\"Shape of tensor patches = {image_tiles.shape}\")\n",
    "    merged_image = merger(image_tiles)\n",
    "    print(f\"Shape of merged image = {merged_image.shape}\")\n",
    "    assert (image == merged_image).all()\n",
    "    return merged_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c9c54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor patches = torch.Size([2, 9, 3, 4, 4])\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Original image size must be divisible by 2. Got (9, 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m9\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mextract_and_combine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mextract_and_combine\u001b[0;34m(image, window_size, padding)\u001b[0m\n\u001b[1;32m      5\u001b[0m image_tiles \u001b[38;5;241m=\u001b[39m tiler(image)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of tensor patches = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_tiles\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m merged_image \u001b[38;5;241m=\u001b[39m \u001b[43mmerger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of merged image = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmerged_image\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (image \u001b[38;5;241m==\u001b[39m merged_image)\u001b[38;5;241m.\u001b[39mall()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspaces/kornia/kornia/contrib/extract_patches.py:143\u001b[0m, in \u001b[0;36mCombineTensorPatches.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcombine_tensor_patches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moriginal_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpadding\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/kornia/kornia/contrib/extract_patches.py:200\u001b[0m, in \u001b[0;36mcombine_tensor_patches\u001b[0;34m(patches, original_size, window_size, stride, unpadding)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly stride == window_size is supported. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstride\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease feel free to drop a PR to Kornia Github.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    197\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m original_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal image size must be divisible by 2. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unpadding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     hpad_check \u001b[38;5;241m=\u001b[39m (original_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m unpadding[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m unpadding[\u001b[38;5;241m3\u001b[39m]) \u001b[38;5;241m%\u001b[39m window_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Original image size must be divisible by 2. Got (9, 9)"
     ]
    }
   ],
   "source": [
    "image = torch.randn(2, 3, 9, 9)\n",
    "_ = extract_and_combine(image, window_size=(4, 4), padding=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f5d2c",
   "metadata": {},
   "source": [
    "To solve this we could pad the image prior to extracting tensor patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86703739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 9, 9])\n",
      "torch.Size([2, 3, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "image = torch.randn(2, 3, 9, 9)\n",
    "print(image.shape)\n",
    "\n",
    "# Pad last two dim by 1\n",
    "padded_image = F.pad(image, (1,0,1,0))\n",
    "print(padded_image.shape)\n",
    "\n",
    "h, w = padded_image.shape[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33823bd",
   "metadata": {},
   "source": [
    "Now that the image dimensions are divisible by 2, let's try extracting and combining tensor patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b5d62c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor patches = torch.Size([2, 9, 3, 4, 4])\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Insufficient padding",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mextract_and_combine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mextract_and_combine\u001b[0;34m(image, window_size, padding)\u001b[0m\n\u001b[1;32m      5\u001b[0m image_tiles \u001b[38;5;241m=\u001b[39m tiler(image)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of tensor patches = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_tiles\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m merged_image \u001b[38;5;241m=\u001b[39m \u001b[43mmerger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of merged image = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmerged_image\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (image \u001b[38;5;241m==\u001b[39m merged_image)\u001b[38;5;241m.\u001b[39mall()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspaces/kornia/kornia/contrib/extract_patches.py:143\u001b[0m, in \u001b[0;36mCombineTensorPatches.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcombine_tensor_patches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moriginal_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpadding\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/kornia/kornia/contrib/extract_patches.py:207\u001b[0m, in \u001b[0;36mcombine_tensor_patches\u001b[0;34m(patches, original_size, window_size, stride, unpadding)\u001b[0m\n\u001b[1;32m    204\u001b[0m     wpad_check \u001b[38;5;241m=\u001b[39m (original_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m unpadding[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m unpadding[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m%\u001b[39m window_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hpad_check \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wpad_check:\n\u001b[0;32m--> 207\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInsufficient padding\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    209\u001b[0m     window_size \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    210\u001b[0m         (original_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m (unpadding[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m unpadding[\u001b[38;5;241m3\u001b[39m])) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m window_size[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    211\u001b[0m         (original_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (unpadding[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m unpadding[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m window_size[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    212\u001b[0m     )\n\u001b[1;32m    214\u001b[0m patches_tensor \u001b[38;5;241m=\u001b[39m patches\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, window_size[\u001b[38;5;241m0\u001b[39m], window_size[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m*\u001b[39mpatches\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:])\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Insufficient padding"
     ]
    }
   ],
   "source": [
    "_ = extract_and_combine(padded_image, window_size=(4,4), padding=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03517c22",
   "metadata": {},
   "source": [
    "Notice that we now run into the second case i.e. padded image should be divisible by `window_size`. From the previous cell:\n",
    "\n",
    "- original_size = (10, 10) # after we padded by 1\n",
    "- window_size = (4, 4)\n",
    "- padding = 2\n",
    "\n",
    "We can indeed verify that (10 + 2 + 2) % 4 != 0. A simple solution would be to reduce padding by 1 which would result in \n",
    "\n",
    "- original_size = (10, 10) # after we padded by 1\n",
    "- window_size = (4, 4)\n",
    "- padding = 1\n",
    "\n",
    "Now that (10 + 1 + 1) % 4 == 0, we should be good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdd32a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor patches = torch.Size([2, 9, 3, 4, 4])\n",
      "Shape of merged image = torch.Size([2, 3, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "prepad_restored_image = extract_and_combine(padded_image, window_size=(4,4), padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad5fcee",
   "metadata": {},
   "source": [
    "Finally, to get back our original image, we simply need to remove the padding that we added earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09e922ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "restored_image = F.pad(prepad_restored_image, (-1,-0,-1,-0))\n",
    "print(restored_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00b323bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that the original image and restored image are the same\n",
    "assert (restored_image == image).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb50bfe",
   "metadata": {},
   "source": [
    "## Rectangular images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62976aa7",
   "metadata": {},
   "source": [
    "These functions also work with rectangular images provided we account for the cases mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b65659e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 8, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "rect_image = torch.randn(1, 1, 8, 6)\n",
    "print(rect_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c95f3da",
   "metadata": {},
   "source": [
    "Notice that the original image dimensions (8, 6) are even so we just need to ensure the padded image is divisible by window size. In this case, the height of the image (8) is already divisible by window height (4). But this is not the case for the image width (6). To fix this, we only need to pad the width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a07b557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor patches = torch.Size([1, 4, 1, 4, 4])\n",
      "Shape of merged image = torch.Size([1, 1, 8, 6])\n"
     ]
    }
   ],
   "source": [
    "restored_image = extract_and_combine(rect_image, window_size=(4,4), padding=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7997ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that the original image and restored image are the same\n",
    "assert (restored_image == rect_image).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
