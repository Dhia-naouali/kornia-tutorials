{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34fb3a3e",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.sandbox.google.com/github/kornia/tutorials/blob/master/source/extract-combine-patches.ipynb)\n",
    "\n",
    "# Extracting and Combining Tensor Patches\n",
    "\n",
    "In this tutorial we will show how you can extract and combine tensor patches using kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611b350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "!pip install git+https://github.com/kornia/kornia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb4bd6d",
   "metadata": {},
   "source": [
    "## Using Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7419ae3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 8, 8])\n",
      "torch.Size([2, 9, 3, 4, 4])\n",
      "torch.Size([2, 3, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from kornia.contrib import CombineTensorPatches, ExtractTensorPatches\n",
    "\n",
    "h, w = 8, 8\n",
    "win = 4\n",
    "pad = 2\n",
    "\n",
    "image = torch.randn(2, 3, h, w)\n",
    "print(image.shape)\n",
    "tiler = ExtractTensorPatches(window_size=win, stride=win, padding=pad)\n",
    "merger = CombineTensorPatches(original_size=(h, w), window_size=win,  unpadding=pad)\n",
    "image_tiles = tiler(image)\n",
    "print(image_tiles.shape)\n",
    "new_image = merger(image_tiles)\n",
    "print(new_image.shape)\n",
    "assert (image == new_image).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e9bd7e",
   "metadata": {},
   "source": [
    "## Using Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ad0bae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 8, 8])\n",
      "torch.Size([1, 9, 1, 4, 4])\n",
      "torch.Size([1, 1, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from kornia.contrib import combine_tensor_patches, extract_tensor_patches\n",
    "\n",
    "h, w = 8, 8\n",
    "win = 4\n",
    "pad = 2\n",
    "\n",
    "image = torch.randn(1, 1, h, w)\n",
    "print(image.shape)\n",
    "patches = extract_tensor_patches(image, window_size=win, stride=win, padding=pad)\n",
    "print(patches.shape)\n",
    "restored_img = combine_tensor_patches(patches, original_size=(h, w), window_size=win,  stride=win, unpadding=pad)\n",
    "print(restored_img.shape)\n",
    "assert (image == restored_img).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc72717",
   "metadata": {},
   "source": [
    "## Important cases to consider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672fb037",
   "metadata": {},
   "source": [
    "While using these functions, it is important to keep track of the following points:\n",
    "\n",
    "1. Original image dimensions prior to extraction must be divisible by 2\n",
    "2. Image after padding must be divisible by window_size \n",
    "3. CombineTensorPatches only works with stride == window_size\n",
    "\n",
    "We will now examine the cases 1 and 2 and how to address them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01add821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_combine(image, window_size, padding):\n",
    "    h, w = image.shape[-2:]\n",
    "    tiler = ExtractTensorPatches(window_size=window_size, stride=window_size, padding=padding)\n",
    "    merger = CombineTensorPatches(original_size=(h, w), window_size=window_size, unpadding=padding)\n",
    "    image_tiles = tiler(image)\n",
    "    print(f\"Shape of tensor patches = {image_tiles.shape}\")\n",
    "    merged_image = merger(image_tiles)\n",
    "    print(f\"Shape of merged image = {merged_image.shape}\")\n",
    "    assert (image == merged_image).all()\n",
    "    return merged_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52c9c54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor patches = torch.Size([2, 9, 3, 4, 4])\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Original image size must be divisible by 2. Got (9, 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1420618/1908982385.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_and_combine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1420618/652039120.py\u001b[0m in \u001b[0;36mextract_and_combine\u001b[0;34m(image, window_size, padding)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimage_tiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of tensor patches = {image_tiles.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmerged_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of merged image = {merged_image.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmerged_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torchgeo/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torchgeo/lib/python3.9/site-packages/kornia/contrib/extract_patches.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         return combine_tensor_patches(\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpadding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/envs/torchgeo/lib/python3.9/site-packages/kornia/contrib/extract_patches.py\u001b[0m in \u001b[0;36mcombine_tensor_patches\u001b[0;34m(patches, original_size, window_size, stride, unpadding)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moriginal_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0moriginal_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Original image size must be divisible by 2. Got {original_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0munpadding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Original image size must be divisible by 2. Got (9, 9)"
     ]
    }
   ],
   "source": [
    "image = torch.randn(2, 3, 9, 9)\n",
    "_ = extract_and_combine(image, window_size=(4, 4), padding=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f5d2c",
   "metadata": {},
   "source": [
    "To solve this we could pad the image prior to extracting tensor patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86703739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 9, 9])\n",
      "torch.Size([2, 3, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "image = torch.randn(2, 3, 9, 9)\n",
    "print(image.shape)\n",
    "\n",
    "# Pad last two dim by 1\n",
    "padded_image = F.pad(image, (1,0,1,0))\n",
    "print(padded_image.shape)\n",
    "\n",
    "h, w = padded_image.shape[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33823bd",
   "metadata": {},
   "source": [
    "Now that the image dimensions are divisible by 2, let's try extracting and combining tensor patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b5d62c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor patches = torch.Size([2, 9, 3, 4, 4])\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Insufficient padding",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1420618/806108492.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_and_combine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1420618/652039120.py\u001b[0m in \u001b[0;36mextract_and_combine\u001b[0;34m(image, window_size, padding)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimage_tiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of tensor patches = {image_tiles.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmerged_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of merged image = {merged_image.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmerged_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torchgeo/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torchgeo/lib/python3.9/site-packages/kornia/contrib/extract_patches.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         return combine_tensor_patches(\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpadding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/envs/torchgeo/lib/python3.9/site-packages/kornia/contrib/extract_patches.py\u001b[0m in \u001b[0;36mcombine_tensor_patches\u001b[0;34m(patches, original_size, window_size, stride, unpadding)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhpad_check\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwpad_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Insufficient padding\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         window_size = (\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Insufficient padding"
     ]
    }
   ],
   "source": [
    "_ = extract_and_combine(padded_image, window_size=(4,4), padding=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03517c22",
   "metadata": {},
   "source": [
    "Notice that we now run into the second case i.e. padded image should be divisible by `window_size`. From the previous cell:\n",
    "\n",
    "- original_size = (10, 10) # after we padded by 1\n",
    "- window_size = (4, 4)\n",
    "- padding = 2\n",
    "\n",
    "We can indeed verify that (10 + 2 + 2) % 4 != 0. A simple solution would be to reduce padding by 1 which would result in \n",
    "\n",
    "- original_size = (10, 10) # after we padded by 1\n",
    "- window_size = (4, 4)\n",
    "- padding = 1\n",
    "\n",
    "Now that (10 + 1 + 1) % 4 == 0, we should be good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdd32a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor patches = torch.Size([2, 9, 3, 4, 4])\n",
      "Shape of merged image = torch.Size([2, 3, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "prepad_restored_image = extract_and_combine(padded_image, window_size=(4,4), padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad5fcee",
   "metadata": {},
   "source": [
    "Finally, to get back our original image, we simply need to remove the padding that we added earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09e922ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "restored_image = F.pad(prepad_restored_image, (-1,-0,-1,-0))\n",
    "print(restored_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00b323bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that the original image and restored image are the same\n",
    "assert (restored_image == image).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb50bfe",
   "metadata": {},
   "source": [
    "## Rectangular images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62976aa7",
   "metadata": {},
   "source": [
    "These functions also work with rectangular images provided we account for the cases mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b65659e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 8, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "rect_image = torch.randn(1, 1, 8, 6)\n",
    "print(rect_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c95f3da",
   "metadata": {},
   "source": [
    "Notice that the original image dimensions (8, 6) are even so we just need to ensure the padded image is divisible by window size. In this case, the height of the image (8) is already divisible by window height (4). But this is not the case for the image width (6). To fix this, we only need to pad the width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a07b557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor patches = torch.Size([1, 4, 1, 4, 4])\n",
      "Shape of merged image = torch.Size([1, 1, 8, 6])\n"
     ]
    }
   ],
   "source": [
    "restored_image = extract_and_combine(rect_image, window_size=(4,4), padding=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7997ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that the original image and restored image are the same\n",
    "assert (restored_image == rect_image).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
